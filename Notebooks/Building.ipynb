{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "BATCH_SIZE = 1\n",
    "FEATURE_SIZE = 784\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('../data', train=True, download=True,\n",
    "               transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))])),batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    input_stimulus = data.view(-1,FEATURE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Forward and Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward(torch.autograd.Function):\n",
    "    def forward(ctx, hidden_state, feed_forward, context=None):\n",
    "        if context is not None:\n",
    "            #print(feed_forward.shape, context.shape,hidden_state.shape)\n",
    "            state = feed_forward * context\n",
    "            result = torch.mm(state,hidden_state)\n",
    "            \n",
    "        else:\n",
    "            result = torch.mm(feed_forward,hidden_state)\n",
    "        ctx.hidden_state = hidden_state\n",
    "        return result\n",
    "\n",
    "    def backward(ctx, grad_forward):\n",
    "        #print('Backward',ctx.hidden_state)\n",
    "        return ctx.hidden_state,grad_forward\n",
    "\n",
    "class Backward(torch.autograd.Function):\n",
    "    def forward(ctx, hidden_state, feed_forward, context=None):\n",
    "        if context is not None:\n",
    "            state = feed_forward * context\n",
    "            result = torch.mm(state,hidden_state)\n",
    "            \n",
    "        else:\n",
    "            result = torch.mm(feed_forward,hidden_state)\n",
    "        ctx.hidden_state = hidden_state\n",
    "        return result\n",
    "\n",
    "    def backward(ctx, grad_forward):\n",
    "        #print('Backward',ctx.hidden_state)\n",
    "        return ctx.hidden_state,grad_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Feedback Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CF(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size=100, dtype=torch.FloatTensor, num_layers=5,weight_transport=True):\n",
    "        super(CF, self).__init__()\n",
    "        self.weight_transport = weight_transport\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # forward weights: 1,2,3,4,5\n",
    "        self.forward_params = nn.ParameterList([nn.Parameter(torch.randn(feature_size, hidden_size).type(dtype))] +\\\n",
    "                                               [nn.Parameter(torch.randn(hidden_size, hidden_size).type(dtype)) for _ in range(num_layers-1)])\n",
    "        if weight_transport: # using same weights for forward and backward pass\n",
    "            self.backward_params = self.forward_params\n",
    "        else:\n",
    "            # backward weights: 5,4,3,2,1\n",
    "            self.backward_params = nn.ParameterList([nn.Parameter(torch.randn(hidden_size, hidden_size).type(dtype)) for _ in range(num_layers-1)]+\\\n",
    "                                [nn.Parameter(torch.randn(hidden_size, feature_size).type(dtype))])\n",
    "        \n",
    "        self.backward_context = []\n",
    "        self.prior_activities = []\n",
    "        self.current_activities = []\n",
    "        #self.cell = nn.GRUCell(input_size=feature_size, hidden_size=hidden_size)\n",
    "        #self.layer_1 = Forward.apply\n",
    "        \n",
    "    def reset_hidden(self):\n",
    "        self.hidden_state = torch.zeros((self.batch_size, self.hidden_size))\n",
    "        \n",
    "    def forward(self, feed_forward):\n",
    "        for seq_idx in range(feed_forward.shape[0]):\n",
    "            forward = feed_forward[seq_idx].view(-1,784)\n",
    "            \n",
    "            # going up\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                layer = self.forward_params[layer_idx]\n",
    "                if seq_idx != 0:\n",
    "                    forward = Forward()(layer, forward, self.backward_context[layer_idx])\n",
    "                else:\n",
    "                    forward = Forward()(layer, forward)\n",
    "\n",
    "                self.current_activities.append(forward)\n",
    "\n",
    "            backward = forward\n",
    "            if self.weight_transport:\n",
    "                start,stop,step = self.num_layers-1,0,-1\n",
    "            else:\n",
    "                start,stop,step = 0,self.num_layers,1\n",
    "            \n",
    "            # going down\n",
    "            for layer_idx in range(start,stop,step):\n",
    "                layer = self.backward_params[layer_idx]\n",
    "                backward = Backward()(layer, backward)\n",
    "                self.backward_context.insert(0,backward)\n",
    "                \n",
    "            self.prior_activities = self.current_activities\n",
    "            \n",
    "        #self.batch_size = feed_forward.size(1)\n",
    "        #self.reset_hidden()\n",
    "        return backward\n",
    "\n",
    "model = CF(FEATURE_SIZE)\n",
    "z = model(input_stimulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one.png.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "# make_dot was moved to https://github.com/szagoruyko/pytorchviz\n",
    "from torchviz import make_dot\n",
    "d = make_dot(model(input_stimulus), params=dict(model.named_parameters()))\n",
    "d.render(filename='one.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
